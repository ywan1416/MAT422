{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPACs1z4hXE/zbvylE+Mtih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywan1416/MAT422/blob/main/HW3_7_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "IrCxOkVVy2wB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.1. Mathematical formulation"
      ],
      "metadata": {
        "id": "2Zi8gPBglZ_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction function is: y=σ(z)=σ(w_1a_1+w_2a_2+b) where σ(z) is the activation function applied to the weighted sum of inputs and bias."
      ],
      "metadata": {
        "id": "l3Pxvh0Xwmpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Learning"
      ],
      "metadata": {
        "id": "UWWdbJ83wqUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks process input data and adjust internal weights to produce outputs that match the target. This iterative adjustment, called supervised learning, continues until specific criteria are met.\n",
        "\n"
      ],
      "metadata": {
        "id": "CpoXlifelzuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function z\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    z = np.dot(weights, x) + bias\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Enter the example data [count,length]\n",
        "x = np.array([3, 1])\n",
        "\n",
        "# Define weights and bias\n",
        "weights = np.array([0.5, -0.3])\n",
        "bias = 0.1\n",
        "\n",
        "# Predict the output\n",
        "output = predict(x, weights, bias)\n",
        "print(f'Predicted output: {output:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkOYiwuSlz6Z",
        "outputId": "f9f1439a-5254-47d8-fde8-de628a77fcb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted output: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.2. Activation functions"
      ],
      "metadata": {
        "id": "mJxzhClUlegb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions process the input received by a node and apply a non-linear transformation to handle complex non-linear tasks."
      ],
      "metadata": {
        "id": "OsgITXkCz9QM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step Function:\n",
        "\n",
        "Expression:\n",
        "σ(x)=0,if x<0;\n",
        "σ(x)=1,if x≥0.\n"
      ],
      "metadata": {
        "id": "qNTVramj3MmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rectified Linear Unit :\n",
        "\n",
        "Expression:\n",
        "σ(x)=max(0,x)"
      ],
      "metadata": {
        "id": "Y_6spIHE3ksi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Function:\n",
        "\n",
        "Expression:\n",
        "σ(x)= 1/(1+e^(−x))"
      ],
      "metadata": {
        "id": "SbbgnJC13qf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Step function\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0"
      ],
      "metadata": {
        "id": "kwEEKubu6Pye"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Rectified Linear Unit\n",
        "def relu(x):\n",
        "    return max(0, x)"
      ],
      "metadata": {
        "id": "wr68ZUy56P5j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "BdMgAGCb6WJE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sample data\n",
        "x1 = 1\n",
        "x2 = 2\n",
        "\n",
        "# Define Weights and bias\n",
        "w1, w2 = 0.5, -1.5\n",
        "bias = 0.1\n",
        "\n",
        "# Calculate the weighted sum\n",
        "z = w1 * x1 + w2 * x2 + bias\n",
        "\n",
        "#ouput\n",
        "output_step = step_function(z)\n",
        "output_relu = relu(z)\n",
        "output_sigmoid = sigmoid(z)\n",
        "\n",
        "print(f'Step Function Output: {output_step}')\n",
        "print(f'ReLU Output: {output_relu}')\n",
        "print(f'Sigmoid Output: {output_sigmoid:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13j4i77T39FG",
        "outputId": "80abcd9c-09f8-4e6a-8228-d63aa5d1b0d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step Function Output: 0\n",
            "ReLU Output: 0\n",
            "Sigmoid Output: 0.083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.3. Cost function"
      ],
      "metadata": {
        "id": "1pidzj0clgoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Least Squares cost function measures the difference between predicted and actual values in a training dataset.\n",
        "\n",
        "Minimizing J reduces prediction errors, enhancing model accuracy."
      ],
      "metadata": {
        "id": "0V88S9306sN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Actual value and predicted value\n",
        "y_act = np.array([2, 3])\n",
        "y_pred = np.array([2.5, 2.8])\n",
        "\n",
        "# Number of training examples\n",
        "N = len(y_act)\n",
        "\n",
        "# Calculate the Least Squares cost function\n",
        "J = (1 / 2) * np.sum((y_pred - y_act) ** 2)\n",
        "\n",
        "print(f'Least Squares Cost Function: {J:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No1PeSM99vNd",
        "outputId": "03088ce7-0095-4cde-9acd-d1ac4e14d9d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Least Squares Cost Function: 0.145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.4. Backpropagation"
      ],
      "metadata": {
        "id": "49nevRNmlkpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation: The process where input data flows through a neural network layer-by-layer, with each layer applying weights, biases, and activation functions, to produce an output prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_QW18HQs_JfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7.5. Backpropagation algorithm"
      ],
      "metadata": {
        "id": "OPBD_wTglnCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize weights and biases, perform forward propagation, calculate the error, backpropagate to find gradients, update weights using an optimizer, and repeat until desired accuracy is reached.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ggd3uO8SDT0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMUsIGImlYjF",
        "outputId": "2ae39c66-289f-452c-c52f-6a0069f19f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights for hidden layer:\n",
            " [[0.20734878 0.41469757]\n",
            " [0.29118146 0.68236292]]\n",
            "Updated biases for hidden layer:\n",
            " [0.10734878 0.19118146]\n",
            "Updated weights for output layer:\n",
            " [ 0.51616732 -0.57207462]\n",
            "Updated bias for output layer:\n",
            " 0.06469756710719123\n"
          ]
        }
      ],
      "source": [
        "# Define functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Initialize weights and biases\n",
        "x = np.array([1, 2])\n",
        "y = 1\n",
        "\n",
        "#hidden layer\n",
        "weights_hidden = np.array([[0.2, 0.4], [0.3, 0.7]])\n",
        "bias_hidden = np.array([0.1, 0.2])\n",
        "\n",
        "#output layer\n",
        "weights_output = np.array([0.5, -0.6])\n",
        "bias_output = 0.05\n",
        "\n",
        "# Forward propagation\n",
        "z_hidden = np.dot(weights_hidden, x) + bias_hidden\n",
        "a_hidden = np.maximum(0, z_hidden)\n",
        "\n",
        "z_output = np.dot(weights_output, a_hidden) + bias_output\n",
        "output = sigmoid(z_output)\n",
        "\n",
        "# Compute the error\n",
        "error = 0.5 * (output - y) ** 2\n",
        "\n",
        "# Backpropagation\n",
        "d_output = (output - y) * sigmoid_derivative(z_output)\n",
        "d_hidden = d_output * weights_output * (z_hidden > 0)\n",
        "\n",
        "# Update weights and biases\n",
        "learning_rate = 0.1\n",
        "weights_output -= learning_rate * d_output * a_hidden\n",
        "bias_output -= learning_rate * d_output\n",
        "\n",
        "weights_hidden -= learning_rate * np.outer(d_hidden, x)\n",
        "bias_hidden -= learning_rate * d_hidden\n",
        "\n",
        "print(\"Updated weights for hidden layer:\\n\", weights_hidden)\n",
        "print(\"Updated biases for hidden layer:\\n\", bias_hidden)\n",
        "print(\"Updated weights for output layer:\\n\", weights_output)\n",
        "print(\"Updated bias for output layer:\\n\", bias_output)\n"
      ]
    }
  ]
}