{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm5slvC1a3mGAgZESUlIgO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywan1416/MAT422/blob/main/HW_2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhAPgfTHWlNR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4.1. MLE for random samples"
      ],
      "metadata": {
        "id": "HW9qS9qbX7q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Maximum Likelihood Estimation (MLE) is a method that uses the probability mass function (pmf) or probability density function (pdf) to find the parameter values that maximize the probability of observing the given data, helping to analyze the numerical distribution."
      ],
      "metadata": {
        "id": "-e05RVN_a_cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get random event A\n",
        "dataA = [0.1, 1.8, 0.3, 1.3, 1.1]\n",
        "print(\"Data:\",dataA)\n",
        "\n",
        "#Find the min and max number\n",
        "min_x = min(dataA)\n",
        "max_x = max(dataA)\n",
        "\n",
        "#Find the range of possible theta values\n",
        "theta_min = max_x - 1\n",
        "theta_max = min_x\n",
        "\n",
        "#Joint pdf function\n",
        "def joint_pdf(theta, data):\n",
        "    # Return 1 if all x in data lie within (theta, theta + 1)\n",
        "    if all(theta <= x <= theta + 1 for x in data):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "candidate_thetas = np.linspace(theta_min, theta_max, 5)\n",
        "\n",
        "for theta in candidate_thetas:\n",
        "    likelihood = joint_pdf(theta, data)\n",
        "    print(f\"Theta: {theta:.2f}, Likelihood: {likelihood}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZxI0mrXYBuU",
        "outputId": "415a5279-346f-4392-d33b-4935a8d0c18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: [0.1, 1.8, 0.3, 1.3, 1.1]\n",
            "Theta: 0.80, Likelihood: 0\n",
            "Theta: 0.62, Likelihood: 0\n",
            "Theta: 0.45, Likelihood: 0\n",
            "Theta: 0.28, Likelihood: 0\n",
            "Theta: 0.10, Likelihood: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4.2. Linear regression"
      ],
      "metadata": {
        "id": "6jZtN31NYCGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the MLE method, the analysis function is transformed into a probability function. By using the mean, variance, and random variables, we obtain the MLE, which corresponds to the least mean square error in linear regression."
      ],
      "metadata": {
        "id": "Ho2ltAKQgua4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get random data x and y\n",
        "x = [random.randint(1, 10) for _ in range(5)]\n",
        "y = [random.randint(1, 10) for _ in range(5)]\n",
        "print(\"Data x:\",x)\n",
        "print(\"Data y:\",y)\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(params):\n",
        "    beta_0, beta_1, sigma = params\n",
        "    x_np = np.array(x)\n",
        "    y_pred = beta_0 + beta_1 * x_np\n",
        "    residuals = y - y_pred\n",
        "    n = len(y)\n",
        "    nll = n / 2 * np.log(2 * np.pi * sigma**2) + np.sum(residuals**2) / (2 * sigma**2)\n",
        "    return nll\n",
        "\n",
        "# Initial guess for beta_0, beta_1, sigma\n",
        "initial_guess = [random.randint(1, 10) for _ in range(3)]\n",
        "\n",
        "# Minimize the log-likelihood\n",
        "result = minimize(log_likelihood, initial_guess, method='L-BFGS-B', bounds=[(-10, 10), (-10, 10), (1e-5, 10)])\n",
        "beta_0_est, beta_1_est, sigma_est = result.x\n",
        "\n",
        "print(\"Estimated beta_0:\",beta_0_est)\n",
        "print(\"Estimated beta_1:\",beta_1_est)\n",
        "print(\"Estimated sigma:\",sigma_est)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olmp2TOr5W55",
        "outputId": "62c4d4d6-3cea-48f2-fe39-c32ce291b0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data x: [1, 5, 8, 10, 10]\n",
            "Data y: [1, 10, 2, 1, 8]\n",
            "Estimated beta_0: 3.5442316973289896\n",
            "Estimated beta_1: 0.12584965946752374\n",
            "Estimated sigma: 3.801816597549927\n"
          ]
        }
      ]
    }
  ]
}